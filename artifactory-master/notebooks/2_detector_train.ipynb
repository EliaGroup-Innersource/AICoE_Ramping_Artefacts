{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../artifactory/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from itertools import repeat\n",
    "from artifact import Saw\n",
    "from data import ArtifactDataset, CachedArtifactDataset\n",
    "from detector import ConvolutionDetector, WindowTransformerDetector\n",
    "from utilities import parameters_k\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# stop warnings\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing hyperparameters as a dictionary, because we can directly log this config dict to W&B.\n",
    "CONFIG = dict(\n",
    "    # width of window\n",
    "    width = 512,\n",
    "    convolution_features=[256, 128, 64, 32],\n",
    "    convolution_width=[5, 9, 17, 33],\n",
    "    convolution_dropout=0.0,\n",
    "    transformer_heads=2,\n",
    "    transformer_feedforward=128,\n",
    "    transformer_layers=2,\n",
    "    transformer_dropout=0,\n",
    "    loss=\"mask\",\n",
    "    loss_boost_fp=0,\n",
    "    \n",
    "    artifact=Saw(min_width=4, max_width=32),\n",
    "    # Optimizer Parameter\n",
    "\n",
    "    # LearningRate Scheduler\n",
    "    \n",
    "    # parameters for study\n",
    "    batch_size = 32, # 'values': [32, 64, 128]\n",
    "    \n",
    "    wandb_group_name = \"test_setup\",\n",
    "    wandb_project_name = \"artifactory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowTransformerDetector_528.96K_18-12-2023_16:34:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = WindowTransformerDetector(window=CONFIG[\"width\"],                    \n",
    "                                  convolution_features=CONFIG[\"convolution_features\"],\n",
    "                                  convolution_width=CONFIG[\"convolution_width\"],\n",
    "                                  convolution_dropout=CONFIG[\"convolution_dropout\"],\n",
    "                                  transformer_heads=CONFIG[\"transformer_heads\"],\n",
    "                                  transformer_feedforward=CONFIG[\"transformer_feedforward\"],\n",
    "                                  transformer_layers=CONFIG[\"transformer_layers\"],\n",
    "                                  transformer_dropout=CONFIG[\"transformer_dropout\"],\n",
    "                                  loss=CONFIG[\"loss\"],\n",
    "                                  loss_boost_fp=CONFIG[\"loss_boost_fp\"])\n",
    "# model = ConvolutionDetector(convolution_features=[128, 64, 32],\n",
    "#                             convolution_width=[5, 9, 33],\n",
    "#                             convolution_dilation=[1, 1, 1],\n",
    "#                             convolution_dropout=0.0,\n",
    "#                             convolution_activation=\"sigmoid\")\n",
    "model_name = f\"{model.__class__.__name__}_{parameters_k(model)}_{datetime.now(pytz.timezone('Europe/Amsterdam')).strftime('%d-%m-%Y_%H:%M:%S')}\"\n",
    "CONFIG['wandb_run_name'] = model_name\n",
    "\n",
    "val_file = Path(f\"../data/validation{CONFIG['width']}.all.pkl\")\n",
    "val_datasets = [\n",
    "    #\"australian_electricity_demand_dataset\",\n",
    "    #\"electricity_hourly_dataset\",\n",
    "    \"electricity_load_diagrams\",\n",
    "    #\"HouseholdPowerConsumption1\",\n",
    "    #\"HouseholdPowerConsumption2\",\n",
    "    #\"london_smart_meters_dataset_without_missing_values\",\n",
    "    #\"solar_10_minutes_dataset\",\n",
    "    #\"wind_farms_minutely_dataset_without_missing_values\",\n",
    "]\n",
    "train_datasets = [\n",
    "    #\"australian_electricity_demand_dataset\",\n",
    "    #\"electricity_hourly_dataset\",\n",
    "    \"electricity_load_diagrams\",\n",
    "    #\"HouseholdPowerConsumption1\",\n",
    "    #\"HouseholdPowerConsumption2\",\n",
    "    #\"london_smart_meters_dataset_without_missing_values\",\n",
    "    #\"solar_10_minutes_dataset\",\n",
    "    #\"wind_farms_minutely_dataset_without_missing_values\",\n",
    "]\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_series(names: list[str], split: str):\n",
    "    series = list()\n",
    "    counts = list()\n",
    "    for name in names:\n",
    "        with open(f\"../data/processed/{name}_{split}.pickle\", \"rb\") as f:\n",
    "            raw = [a for a in pickle.load(f) if len(a) > CONFIG[\"width\"]]\n",
    "            series.extend(np.array(a).astype(np.float32) for a in raw)\n",
    "            counts.extend(repeat(1 / len(raw), len(raw)))\n",
    "    counts = np.array(counts)\n",
    "    return series, counts / counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_data, train_weights = load_series(train_datasets, \"TRAIN\")\n",
    "train_dataset = ArtifactDataset(train_data,\n",
    "                                CONFIG[\"artifact\"],\n",
    "                                width=CONFIG[\"width\"],\n",
    "                                padding=64,\n",
    "                                weight=train_weights) \n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "if not val_file.exists():\n",
    "    val_data, val_weights = load_series(val_datasets, \"TEST\")\n",
    "    val_gen = ArtifactDataset(val_data,\n",
    "                              CONFIG[\"artifact\"],\n",
    "                              width=CONFIG[\"width\"],\n",
    "                              padding=64,\n",
    "                              weight=val_weights)\n",
    "    val = CachedArtifactDataset.generate(val_gen,\n",
    "                                         n=2048,\n",
    "                                         to=val_file)\n",
    "else:\n",
    "    val = CachedArtifactDataset(file=val_file)\n",
    "val_loader = DataLoader(val, batch_size=CONFIG[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4218, 0.4115, 0.4424,  ..., 0.2263, 0.4321, 0.3807],\n",
       "        [0.4962, 0.4962, 0.5077,  ..., 0.5231, 0.5269, 0.5346],\n",
       "        [0.6597, 0.5825, 0.5357,  ..., 0.2591, 0.2871, 0.3747],\n",
       "        ...,\n",
       "        [0.1793, 0.2249, 0.2006,  ..., 0.4357, 0.4204, 0.4143],\n",
       "        [0.0846, 0.0923, 0.1000,  ..., 0.0615, 0.0769, 0.0769],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.1379, 0.1724, 0.1379]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch[\"data\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9lnh11bk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.0.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.0.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.2.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.2.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.4.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.4.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.6.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/convolutions.6.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/linear.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/linear.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear1.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear1.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear2.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear2.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm1.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm1.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm2.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm2.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.in_proj_bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.in_proj_weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.out_proj.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.out_proj.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear1.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear1.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear2.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear2.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm1.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm1.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm2.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm2.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.in_proj_bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.in_proj_weight</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.out_proj.bias</td><td>▁</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.out_proj.weight</td><td>▁</td></tr><tr><td>grad_2.0_norm_total</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>train</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>grad_2.0_norm/convolutions.0.bias</td><td>0.0</td></tr><tr><td>grad_2.0_norm/convolutions.0.weight</td><td>0.0</td></tr><tr><td>grad_2.0_norm/convolutions.2.bias</td><td>1e-05</td></tr><tr><td>grad_2.0_norm/convolutions.2.weight</td><td>0.00019</td></tr><tr><td>grad_2.0_norm/convolutions.4.bias</td><td>7e-05</td></tr><tr><td>grad_2.0_norm/convolutions.4.weight</td><td>0.00174</td></tr><tr><td>grad_2.0_norm/convolutions.6.bias</td><td>0.00118</td></tr><tr><td>grad_2.0_norm/convolutions.6.weight</td><td>0.01579</td></tr><tr><td>grad_2.0_norm/linear.bias</td><td>0.04066</td></tr><tr><td>grad_2.0_norm/linear.weight</td><td>0.22284</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear1.bias</td><td>0.00297</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear1.weight</td><td>0.01551</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear2.bias</td><td>0.00708</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.linear2.weight</td><td>0.03524</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm1.bias</td><td>0.00555</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm1.weight</td><td>0.00448</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm2.bias</td><td>0.00928</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.norm2.weight</td><td>0.00779</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.in_proj_bias</td><td>0.00285</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.in_proj_weight</td><td>0.01469</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.out_proj.bias</td><td>0.00496</td></tr><tr><td>grad_2.0_norm/transformer.layers.0.self_attn.out_proj.weight</td><td>0.02162</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear1.bias</td><td>0.00748</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear1.weight</td><td>0.04099</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear2.bias</td><td>0.01817</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.linear2.weight</td><td>0.09393</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm1.bias</td><td>0.0124</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm1.weight</td><td>0.01057</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm2.bias</td><td>0.02407</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.norm2.weight</td><td>0.02116</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.in_proj_bias</td><td>0.00438</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.in_proj_weight</td><td>0.02238</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.out_proj.bias</td><td>0.00872</td></tr><tr><td>grad_2.0_norm/transformer.layers.1.self_attn.out_proj.weight</td><td>0.03616</td></tr><tr><td>grad_2.0_norm_total</td><td>0.26097</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train</td><td>0.04402</td></tr><tr><td>trainer/global_step</td><td>49</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">WindowTransformerDetector_528.96K_18-12-2023_16:34:18</strong> at: <a href='https://wandb.ai/plant_pathology/artifactory/runs/9lnh11bk' target=\"_blank\">https://wandb.ai/plant_pathology/artifactory/runs/9lnh11bk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231218_153522-9lnh11bk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9lnh11bk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/AICoE_Ramping_Artefacts/artifactory-master/notebooks/wandb/run-20231218_153754-tnq0b0l7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hvonhue/artifactory/runs/tnq0b0l7' target=\"_blank\">WindowTransformerDetector_528.96K_18-12-2023_16:34:18</a></strong> to <a href='https://wandb.ai/hvonhue/artifactory' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hvonhue/artifactory' target=\"_blank\">https://wandb.ai/hvonhue/artifactory</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hvonhue/artifactory/runs/tnq0b0l7' target=\"_blank\">https://wandb.ai/hvonhue/artifactory/runs/tnq0b0l7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize W&B run\n",
    "run = wandb.init(project=CONFIG[\"wandb_project_name\"], \n",
    "        config=CONFIG,\n",
    "        entity=\"hvonhue\",\n",
    "        group=CONFIG[\"wandb_group_name\"], \n",
    "        job_type='train',\n",
    "        name=CONFIG[\"wandb_run_name\"])\n",
    "\n",
    "wandb.config.type = 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name         | Type                        | Params\n",
      "-------------------------------------------------------------\n",
      "0 | convolutions | Sequential                  | 503 K \n",
      "1 | position     | SinusoidalPositionEmbedding | 0     \n",
      "2 | dropout      | Dropout                     | 0     \n",
      "3 | transformer  | TransformerEncoder          | 25.4 K\n",
      "4 | linear       | Linear                      | 33    \n",
      "-------------------------------------------------------------\n",
      "528 K     Trainable params\n",
      "0         Non-trainable params\n",
      "528 K     Total params\n",
      "2.116     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 1505/? [32:32<00:00,  0.77it/s, v_num=b0l7]          "
     ]
    }
   ],
   "source": [
    "# initialize callbacks\n",
    "checkpointcallback = ModelCheckpoint(monitor=\"validation\",\n",
    "                                     mode=\"min\",\n",
    "                                     save_top_k=1)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# initialize logger\n",
    "logger = WandbLogger(project=\"artifactory\",\n",
    "                     name=model_name,\n",
    "                     log_model=\"all\")\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(logger=logger,\n",
    "                  max_steps=50000,\n",
    "                  val_check_interval=1000,\n",
    "                  callbacks=[checkpointcallback,\n",
    "                             lr_monitor])\n",
    "\n",
    "# train\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Wandb run\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
